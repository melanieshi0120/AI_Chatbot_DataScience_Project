{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "config_dict = {\n",
    "    'client_id': config.get('reddit', 'ID'),\n",
    "    'client_secret': config.get('reddit', 'SECRET'),\n",
    "    'username': config.get('reddit', 'USERNAME'),\n",
    "    'password': config.get('reddit', 'PASSWORD'),\n",
    "    'user_agent': config.get('reddit', 'AGENT'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAScraper:\n",
    "    def __init__(self):\n",
    "        self.reddit = None\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        \n",
    "    def set_reddit(self, config):\n",
    "        '''\n",
    "        Method to use reddit to get questions and answers for topics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config: Dictionary of containing client_id, client_secret, username, password, and user_agent\n",
    "        '''\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id = config['client_id'],\n",
    "            client_secret = config['client_secret'],\n",
    "            username = config['username'],\n",
    "            password = config['password'],\n",
    "            user_agent = config['user_agent'],\n",
    "        )\n",
    "    \n",
    "    # Helper function to extract posts in a single subreddit\n",
    "    def _extract_reddit(self, sub):\n",
    "        subreddit = self.reddit.subreddit(sub)\n",
    "        hot = subreddit.hot(limit=100)\n",
    "        \n",
    "        title_reply = []\n",
    "        for submission in hot:\n",
    "            if not submission.stickied:\n",
    "                comments = submission.comments\n",
    "                for comment in comments:\n",
    "                    try:\n",
    "                        title_reply.append({\n",
    "                            'body': comment.body,\n",
    "                            'reply': [ reply.body for reply in comment.replies ],\n",
    "                        })                 \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for i in range(len(title_reply)):\n",
    "            questions.append(list(title_reply)[i]['body'])\n",
    "            answers.append(list(title_reply)[i]['reply']) \n",
    "\n",
    "        return questions, answers\n",
    "    \n",
    "    def get_reddit(self, subs):\n",
    "        '''\n",
    "        Method to add questions and answers from list of subreddits\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        subs: List of subreddits (strings) for getting posts\n",
    "        '''\n",
    "        assert type(subs) == list, 'Subreddits must be list of strings'\n",
    "        \n",
    "        success = []\n",
    "        failure = []\n",
    "        for sub in subs:\n",
    "            try:\n",
    "                q, a = self._extract_reddit(sub)\n",
    "                success.append(f'{sub}')\n",
    "                self.questions.append(q)\n",
    "                self.answers.append(a)\n",
    "            except:\n",
    "                failure.append(f'{sub}')\n",
    "                \n",
    "        print('Subreddits searched:', ', '.join(success))\n",
    "        print('Subreddits not searched:', ', '.join(failure))\n",
    "        return\n",
    "    \n",
    "    # Helper function to get all hrefs on a page\n",
    "    def _href(self, soup):\n",
    "        hrefs = []\n",
    "        for i in soup.find_all('a', class_='question-hyperlink', href=True):\n",
    "            hrefs.append(i['href'])\n",
    "            \n",
    "        return hrefs\n",
    "    \n",
    "    # Helper function to collect all valid hrefs\n",
    "    def _clean_empty_hrefs(self, hrefs):\n",
    "        # Remove all empty lists\n",
    "        list_hrefs = []\n",
    "        for i in hrefs:\n",
    "            if i != []:\n",
    "                list_hrefs.append(i)\n",
    "                \n",
    "        # Merge all elements in one list\n",
    "        hrefs_list = []\n",
    "        for i in list_hrefs:\n",
    "            for j in i:\n",
    "                hrefs_list.append(j)\n",
    "                \n",
    "        return hrefs_list\n",
    "    \n",
    "    # Helper function to correct addresses\n",
    "    def _add_prefix(self, hrefs_list):\n",
    "        # Arrange links that do not have 'https://stackoverflow.com' prefix\n",
    "        new_href = []\n",
    "        prefix = 'https://stackoverflow.com'\n",
    "        for h in hrefs_list:\n",
    "            if 'https' not in h:\n",
    "                m = prefix + h + 'answertab=votes#tab-top'\n",
    "                new_href.append(m)\n",
    "            else:\n",
    "                new_href.append(h + 'answertab=votes#tab-top')\n",
    "            \n",
    "        return new_href\n",
    "    \n",
    "    # Helper function to obtain one soup of one question \n",
    "    def _single_page_scraper(self, url):\n",
    "        r = requests.get(url=url)\n",
    "        soup = bs(r.text, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    # Helper function to obtain the questions and answers of a url\n",
    "    def _single_page_question_answer(self, url):\n",
    "        page = self._single_page_scraper(url).find_all('div', class_='post-layout')\n",
    "        questions = [ i.find('p').get_text() for i in page ][0]\n",
    "        answers = [ i.find('p').get_text() for i in page ][1]\n",
    "\n",
    "        return questions, answers\n",
    "    \n",
    "    def get_stack_overflow(self, maximum_pages, topics):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        maximum_pages: Number (int) of pages to scrape\n",
    "        topics: List of topics (strings) to query\n",
    "        '''\n",
    "        assert type(topics) == list, 'Topics must be list of strings'\n",
    "        for topic in topics:\n",
    "            soups = []\n",
    "            for page in range(1, maximum_pages + 1):\n",
    "                url = f'https://stackoverflow.com/questions/tagged/{topic}?tab=votes&page={page}&pagesize=15'\n",
    "                r = requests.get(url=url)\n",
    "                soup = bs(r.text, 'html.parser')\n",
    "                soups.append(soup)\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "            print(f'{topic.capitalize()} soups are ready!')\n",
    "                  \n",
    "            # Obtain and process all hrefs\n",
    "            hrefs = [ self._href(soup) for soup in soups ]\n",
    "            hrefs_list = self._clean_empty_hrefs(hrefs)\n",
    "            new_hrefs_list = self._add_prefix(hrefs_list)\n",
    "            print(f'All {topic.capitalize()} hrefs are ready!')\n",
    "            \n",
    "            # Retrieve and append questions and answers from each page\n",
    "            for url in new_hrefs_list:\n",
    "                time.sleep(0.2)\n",
    "                try:\n",
    "                    q, a = self._single_page_question_answer(url)\n",
    "                    self.questions.append(q)\n",
    "                    self.answers.append(a)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print(f'{topic.capitalize()} finished!')\n",
    "        \n",
    "        assert len(self.questions) == len(self.answers)\n",
    "        print('All topics finished!')\n",
    "        return\n",
    "    \n",
    "    def write_csv(self, name):\n",
    "        '''\n",
    "        Method to write csv file of questions and answers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name: Name of file (string)\n",
    "        '''\n",
    "        df = pd.DataFrame([self.questions, self.answers], index=['questions', 'answers']).T\n",
    "        df.to_csv(f'{name}')\n",
    "        print(f'{name} made!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = QAScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python soups are ready!\n",
      "All Python hrefs are ready!\n",
      "Python finished!\n",
      "All topics finished!\n"
     ]
    }
   ],
   "source": [
    "scraper.get_stack_overflow(1, ['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eggs.csv made!\n"
     ]
    }
   ],
   "source": [
    "scraper.write_csv('eggs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
